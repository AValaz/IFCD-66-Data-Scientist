# -*- coding: utf-8 -*-
"""TITANIC1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eWCzbxkCgc8FmOWjUTtxhwvfFSCE_YGE

# EJEMPLO DE EDA
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import numpy as np
from sklearn.cluster import KMeans
from statsmodels.stats.proportion import proportions_ztest

df = pd.read_csv("C:\\Users\\Ale\\Documents\\IronHack\\Git\\salidas\\titanic.csv")
# leer archivo - inclinacion de la barra

"""# Analisis estadistico de las variables

# CUADRO GENERAL
"""

# print(df.columns)
# # ver columnas

# print(df.shape)
# #dimensioni DF . il primo numero indica il numero di file, il secondo di colonne

# print(df.dtypes)
# #tipos de variables

# print(df.select_dtypes(include = ['object']).shape[1])

# print(round(df['Survived'].value_counts(normalize=True)[0],4)*100)

"""# VARIABLE OBJETIVO - SURVIVED"""

# sns.countplot(x = 'Survived', data = df)
# plt.title('Proporción de fallecidos y supervivientes')
# plt.xlabel('0=Muerto 1=Superviviente')
# plt.ylabel('Num. Personas')
# plt.xticks([0,1], ['Muerto', 'Superviviente'])
# # plt.show()

# # Contraste de hipótesis: binomial
# # Variable dicotomica: se requiere contraste de hypotesis que confirme que se trate de una distribucion binomial
# # Hipotesis nula: la distribución de los datos sigue una distribución binomial
# proporcion_supervivientes = df['Survived'].mean()
# n_total = df['Survived'].shape[0]
# n_supervivientes = df['Survived'].sum()
# # print(stats.binomtest(n_supervivientes, n_total, proporcion_supervivientes, alternative = 'two-sided').pvalue) #p-value = 1. Por tanto, aceptamos la hipótesis nula de que si que es una binomial
# # print(stats.binomtest(n_supervivientes, n_total, proporcion_supervivientes, alternative = 'two-sided').pvalue<0.05)

# """# VARIABLES INDEPENDIENTES

# # AGE
# """

# # VARIABLE FRACCIONARIA Y DE INTERVALO. Los con 0 años, se indican los meses

# # print(df['Age'].value_counts(dropna=False))
# # value_counts contar la frecuencia de valores únicos en una columna
# # con el argumento dropna=False también cuenta las ocurrencias de valores nulos (NaN)

# # Calcular e imprimir la proporción de valores nulos respecto al número de valores no nulos
# # print(df['Age'].isnull().sum()/df['Age'].count())
# # Calcular e imprimir la proporción de valores nulos respecto al total de valores
# # print(df['Age'].isnull().sum()/df['Age'].shape[0])
# # Calcular e imprimir el número de valores nulos
# # print(df['Age'].isnull().sum())

# # en el caso concreto el numero de valores missing es muy alto

# #minimo y maximo
# # print(df['Age'].max())
# # print(df['Age'].min())

# # Curtosis
# # medida estadística que describe la forma de la distribución de los datos

# #Tipo de kurtosis
# # con esta formula se ve el tipo de curtosis. El tipo de distribucion se tiene que confirmar de todas formas con el contraste de hipotesis.
# # print(df['Age'].kurtosis())

# # print(df['Age'].skew())
# # nos indica la asimetria de las colas
# # nos indica hacia donde se desplaza la media
# # Asimetría positiva (skewness > 0) = la cola de la distribución se extiende más hacia la derecha (presencia de atipicos)
# # Asimetría negativa (skewness < 0) = la cola de la distribución se extiende más hacia la izquierda (presencia de atipicos)
# # Asimetría nula (skewness ≈ 0) = la distribución es aproximadamente simétrica
# # = la media se despaza a dx o sx respeto a la mediana

# #Caja y bigotes

# plt.subplot(1,2,1)
# sns.boxplot(x='Age', data = df)
# plt.title("Día caja y bigotes de Age")
# plt.xlabel('Edad en años')

# plt.subplot(1,2,2)
# sns.histplot(x=df['Age'], bins = 30, kde = True)
# plt.title("Histograma con KDE de Age")
# plt.xlabel('Edad en años')
# plt.ylabel("Frecuencia")

# plt.tight_layout()
# # plt.show()

# # En el contexto de visualización de datos, KDE se utiliza para crear una curva suave que representa la distribución de los datos en lugar de un histograma que muestra la distribución en forma de barras discretas.

# # Sabemos que la distrib. tiene atipicos a la derecha. Para individuar los valores atipicos:
# IQR = df['Age'].quantile(0.75)-df['Age'].quantile(0.25)
# #IQR rango intercuartilico (Q3-Q1:describe el rango dentro del cual se encuentra el 50% central de los datos)
# LS = df['Age'].quantile(0.75)+1.5*IQR
# LI = df['Age'].quantile(0.25)-1.5*IQR
# # El factor 1.5 se usa para identificar outliers
# # LS = Q3 +1.5*IQR e  LI = Q1 -1.5*IQR

# # print(df[df['Age']>LS][['PassengerId','Name', 'Age']])                  #info que quiero saber de los outliers
# out = df[df['Age']>LS][['PassengerId','Name', 'Age']]

# plt.scatter(out.index, out['Age'], color='red', label = 'outlier')
# plt.legend()
# # plt.show()

# # contraste de hipótesis
# stat, p = stats.shapiro(df['Age'].dropna())

# plt.figure(figsize=(4,4))
# stats.probplot(df['Age'].dropna(), dist = 'norm', plot = plt)
# # plt.show()

# # print(stat)
# # print (p)                            #la muestra no sigue una distribucion normal

# # test de Shapiro-Wilk: se utiliza para verificar que se trate de una distribucion normal
# # Hipótesis Nula (H0): la muestra proviene de una distribución normal.
# # Hipótesis Alternativa (H1): la muestra no proviene de una distribución normal.

# # stats.probplot es una función en la biblioteca scipy.stats que se utiliza para crear un gráfico de probabilidad.
# # el fin es evaluar si una muestra de datos sigue una distribución específica.
# # dist='norm': especifica la distribución teórica con la que se compararán los datos. En este caso, 'norm' indica la distribución normal.

# # Clusterización (clusterización de los datos de edad (Age) en dos grupos utilizando el algoritmo KMeans)
# # from sklearn.cluster import KMeans
# kmeans = KMeans(n_clusters = 2, random_state=0).fit(df['Age'].dropna().values.reshape(-1,1))
# # n_clusters indica numero de grupos.
# # el parámetro random_state=0 en el contexto de la función KMeans se utiliza para asegurar que los resultados de la clusterización sean reproducibles. Esto significa que cada vez que ejecutes el código con el mismo conjunto de datos y el mismo valor de random_state, obtendrás los mismos clusters.
# # elf valor 0 es solo una semilla para el generador de números aleatorios; podrías usar cualquier otro número entero para el mismo propósito.
# # fit() se utiliza para entrenar el modelo.
# # los valores de la columna Age se transforman en un array bidimensional usando reshape(-1,1) porque KMeans requiere una matriz de entrada de dos dimensiones.
# labels = kmeans.labels_              # Obtener las etiquetas de los clusters
# df['AgeCluster'] = pd.Series([np.nan]*len(df))
# df.loc[~df['Age'].isnull(),'AgeCluster'] = labels

# grupo1 = df.loc[df['AgeCluster']==0, 'Age'].dropna()                                             #G1 y G2 representan los dos grupos creados por la clusterización.
# grupo2 = df.loc[df['AgeCluster']==1, 'Age'].dropna()
# # print(f"G1: x:{grupo1.mean():.2f} - me:{grupo1.median():.2f} - std:{grupo1.std():.2f}")              #estadisticos importantes de los dos clusters
# # print(f"G1: x:{grupo2.mean():.2f} - me:{grupo2.median():.2f} - std:{grupo2.std():.2f}")

# df_c = df.dropna(subset = ['Age']).copy()
# # df.dropna(subset=['Age']) - elimina las filas del DataFrame df donde el valor de la columna 'Age' es NaN (valores faltantes)
# # .copy(): Crea una copia del DataFrame resultante después de eliminar las filas con valores nulos. Esto asegura que df_c sea independiente de df y cualquier modificación en df_c no afectará a df directamente.
# df_c['AgeCluster'] = kmeans.labels_
# # df_c['AgeCluster']: Crea una nueva columna llamada 'AgeCluster' en el DataFrame df_c.
# # kmeans.labels_: Asigna las etiquetas de clusters calculadas por KMeans a la columna 'AgeCluster' del DataFrame df_c.

# plt.figure(figsize=(15,5))

# plt.subplot(1,2,1)
# sns.boxplot(x='AgeCluster', y = 'Age', data = df_c)
# plt.title("Edad por cluster")
# plt.xlabel('Cluster de edad')
# plt.xlabel('Edad en años')

# plt.subplot(1,2,2)
# sns.histplot(data = df_c, x = 'Age', hue = 'AgeCluster', kde=True, bins = 20)
# plt.title("Histograma de clusters")
# plt.xlabel('Edad en años')
# plt.xlabel('Frecuencia')

# # plt.show()

# # Test de proporciones
# # relaciona las dos variables
# tasa_superviviencia = df.groupby('AgeCluster')['Survived'].mean()    # calcula la media de la variable Survived (probabilidad de supervivencia) para cada grupo definido por AgeCluster.
# # print(tasa_superviviencia)

# n1 = len(df[df['AgeCluster']==0])                           # calcula el número de observaciones en el grupo 0.
# n2 = len(df[df['AgeCluster']==1])                           # calcula el número de observaciones en el grupo 1.
# # n1 = df[df['AgeCluster']==0].shape[0]  # Número de observaciones en el grupo 0 -METODO ALTERNATIVO
# # n2 = df[df['AgeCluster']==1].shape[0]  # Número de observaciones en el grupo 1
# s1 = df[df['AgeCluster']==0]['Survived'].sum()              # calcula el número total de supervivientes en el grupo 0.
# s2 = df[df['AgeCluster']==1]['Survived'].sum()              # calcula el número total de supervivientes en el grupo 1.

# c = np.array([s1,s2])                                      # array que contiene el número de supervivientes en cada grupo.
# n = np.array([n1, n2])                                     # array que contiene el tamaño de muestra en cada grupo.

# z_stat, p_value = proportions_ztest(c, n)
# # print(z_stat)
# # print(p_value)                                            # aceptamos H0: los dos promedios son estadisticamente iguales. La edad no influye sobre la tasa de sup.

# """# SEXO"""

# # print(df['Sex'].value_counts(normalize=True).iloc[0])
# # sns.countplot(data=df, x ='Sex')
# # plt.show()

# # Tablas de contigencia
# # print(pd.crosstab(df['Sex'], df['Survived'], margins=True, normalize='index').round(4)*100)


# ## FARE 'data 2.zip'
# # print(df['Fare'].value_counts(dropna=False))      
# # Calcular e imprimir la proporción de valores nulos respecto al número de valores no nulos
# # print(df['Fare'].isnull().sum()/df['Fare'].count())
# # # Calcular e imprimir la proporción de valores nulos respecto al total de valores
# # print(df['Fare'].isnull().sum()/len(df['Fare']))
# # # Calcular e imprimir el número de valores nulos
# # print(df['Fare'].isnull().sum())

# #minimo y maximo
# # print(df['Fare'].max())    
# # print(df['Fare'].min())

# # con esta formula se ve el tipo de curtosis. El tipo de distribucion se tiene que confirmar de todas formas con el contraste de hipotesis. 
# # print(df['Fare'].kurtosis()) # kurtosis = 33.40. Hay una gran cantidad de valores extremos o outliers.
# # print(df['Fare'].skew()) # skew = 4.79. Fuerte asimetría positiva, la mayoría de los valores de "Fare" son relativamente bajos, pero hay algunos valores extremadamente altos que están tirando de la cola de la distribución hacia la derecha.

#Caja y bigotes
# plt.subplot(1,2,1)
# sns.boxplot(x='Fare', data = df)
# plt.title("caja y bigotes de Fare")
# plt.xlabel('Dolares')

# plt.subplot(1,2,2)
# sns.histplot(x=df['Fare'], bins = 30, kde = True)
# plt.title("Histograma con KDE de Fare")
# plt.xlabel('Dolares')
# plt.ylabel("Frecuencia")

# plt.tight_layout()
# plt.show()

## Eliminar los quartiles
IQR = df['Fare'].quantile(0.75)-df['Fare'].quantile(0.25)         
#IQR rango intercuartilico (Q3-Q1:describe el rango dentro del cual se encuentra el 50% central de los datos)
LS = df['Fare'].quantile(0.75)+1.5*IQR
LI = df['Fare'].quantile(0.25)-1.5*IQR
# El factor 1.5 se usa para identificar outliers
# LS = Q3 +1.5*IQR e  LI = Q1 -1.5*IQR

df_fare_cleaned = df[(df['Fare'] >= LI) & (df['Fare'] <= LS)]
# plt.subplot(1,2,1)
# sns.boxplot(x='Fare', data = df_fare_cleaned)
# plt.title("caja y bigotes de Fare")
# plt.xlabel('Dolares')

# plt.subplot(1,2,2)
# sns.histplot(x=df_fare_cleaned['Fare'], bins = 30, kde = True)
# plt.title("Histograma con KDE de Fare")
# plt.xlabel('Dolares')
# plt.ylabel("Frecuencia")
# plt.show()

# contraste de hipótesis
# stat, p = stats.shapiro(df['Fare'].dropna())

# plt.figure(figsize=(4,4))
# stats.probplot(df['Fare'].dropna(), dist = 'norm', plot = plt)
# plt.show()

# print(stat)
# print (p) # No es una distribución normal

# División de los datos
X = df_fare_cleaned.drop('Fare',axis=1).values
y = df_fare_cleaned['Fare'].values
print(X, y)

# from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)
# X_train = np.asarray(X_train, dtype=np.float32)
# y_train = np.asarray(y_train, dtype=np.float32)
# X_test = np.asarray(X_test, dtype=np.float32)
# y_test = np.asarray(y_test, dtype=np.float32)

# from sklearn.preprocessing import MinMaxScaler

# # Estandarizamos los datos
# scaler = MinMaxScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.fit_transform(X_test)

# #Entrenar modelo
# import tensorflow as tf
# from tensorflow import Sequential 
# from tensorflow import Dense, Activation, Dropout 

# model = Sequential()
# model.add(Dense(13,activation='relu'))
# model.add(Dense(6, activation='relu'))
# model.add(Dense(1, activation='sigmoid'))
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), verbose=1)
